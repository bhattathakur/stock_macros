{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrapping on tradingview.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "#import re\n",
    "import csv\n",
    "#from io import StringIO\n",
    "#import json\n",
    "from IPython.core.interactiveshell import InteractiveShell #multiple outputs for a cell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"            #multiple outputs for a cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different categories of interest and url template\n",
    "categories=['large-cap','active','gainers','losers','most-volatile','overbought','oversold']\n",
    "choice=categories[2]\n",
    "url=\"https://www.tradingview.com/markets/stocks-usa/market-movers-\"+\"{}\"+\"/\"\n",
    "response=requests.get(url.format(choice))        #request for a web page\n",
    "soup=BeautifulSoup(response.text,'html.parser')  #creating the soup object with given page\n",
    "#date='052821'\n",
    "# filesuffix=date+'tv.csv'\n",
    "#file=choice+\"tv\"+date+\".csv\"\n",
    "#csv_file=open(file,\"w\")\n",
    "#print(\"csv-file-name\\t\",file)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the ticker block\n",
    "#function to return .csv file for given category\n",
    "url=\"https://www.tradingview.com/markets/stocks-usa/market-movers-\"+\"{}\"+\"/\"\n",
    "def get_csv(category,filesuffix,root):\n",
    "    '''\n",
    "    reutrns the csv file for given category using tradingview.com website for particular day\n",
    "    '''\n",
    "    \n",
    "    response=requests.get(url.format(category))        #request for a web page\n",
    "    soup=BeautifulSoup(response.text,'html.parser')    #parse the webpage\n",
    "    \n",
    "    #header of table\n",
    "    header=['TICKER','LAST','CHG%','CHG','RATING','VOL','MKT CAP','P/E','EPS(TTM)','EMPLOYEES','SECTOR','INFO'] #columns name\n",
    "    \n",
    "    #csv file to store the header and data\n",
    "    file=root+category+filesuffix\n",
    "    csv_file=open(file,\"w\")\n",
    "    #print(\"csv-file-name\\t\",file)\n",
    "    csv_writer=csv.writer(csv_file)\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    table=soup.find('table')#table in webpage\n",
    "    table_body=table.find('tbody',class_=\"tv-data-table__tbody\") #table row\n",
    "    row_data=table_body.find_all('tr') #row data\n",
    "    all_data=[]\n",
    "    web_page='https://www.tradingview.com'\n",
    "    for tr in row_data:\n",
    "        tds=tr.find_all('td')\n",
    "        td_list=[]\n",
    "        for i,td in enumerate(tds):\n",
    "            #print(i,td)\n",
    "            #print(\"=======================\\n\")\n",
    "            if i==0:\n",
    "                \n",
    "                #print(30*\"x\")\n",
    "                symlink=web_page+td.find('a').get('href')\n",
    "                ticker=td.find('a').contents[0]\n",
    "                #print(\"link\\t\",symlink)\n",
    "                #print(\"TICKER\\t\",ticker)\n",
    "                #print()\n",
    "                #add values to the list\n",
    "                td_list.append(ticker)\n",
    "                td_list.append(symlink)\n",
    "            else:\n",
    "                #td_list.append(td.text)\n",
    "                td_list.insert(i,td.text)\n",
    "        all_data.append(td_list)\n",
    "    #print(all_data)\n",
    "    for i in range(len(all_data)):\n",
    "        #print(all_data[i])\n",
    "        csv_writer.writerow(all_data[i])\n",
    "    csv_file.close()\n",
    "    print(\"successfully created file: {}\".format(file))\n",
    "    return file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root='/home/thakur/cks/tvfiles/'                         #dir to save the file\n",
    "# date=datetime.date.today().strftime('%m%d%y')            #today\n",
    "# filesuffix=date+'tv.csv'\n",
    "# act_file=get_csv('overbought',\"oversold.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get csv files for all categories\n",
    "sub=1\n",
    "root='/home/thakur/cks/tvfiles/'                         #dir to save the file\n",
    "date=datetime.date.today()\n",
    "#date=datetime.date.today().strftime('%m%d%y')            #today\n",
    "#if date is not update\n",
    "days_sub=datetime.timedelta(days=sub)                         #subtract 1 day\n",
    "date=date-days_sub\n",
    "date=date.strftime('%m%d%y')  \n",
    "filesuffix=date+'tv.csv'\n",
    "cat=['large-cap','active','gainers','losers','most-volatile','overbought','oversold']\n",
    "#filesuffix=[root+i for i in cat]\n",
    "csv_file_list=[]\n",
    "for category in cat:\n",
    "    fi=get_csv(category,filesuffix,root)\n",
    "    csv_file_list.append(fi)\n",
    "    #print('file\\t',fi)\n",
    "#print(\"file list\\n {}\".format(csv_file_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get csv files for all categories\n",
    "# root='/home/thakur/cks/tvfiles/'                         #dir to save the file\n",
    "# date=datetime.date.today().strftime('%m%d%y')            #today\n",
    "# filesuffix=date+'tv.csv'\n",
    "# categories=['large-cap','active','gainers','losers','most-volatile','overbought','oversold']\n",
    "# categories=[root+i for i in categories]\n",
    "# csv_file_list=[]\n",
    "# for category in categories:\n",
    "#     file=root+category+filesuffix\n",
    "#     fi=get_csv(category,filesuffix)\n",
    "#     csv_file_list.append(fi)\n",
    "#     #print('file\\t',fi)\n",
    "# #print(\"file list\\n {}\".format(csv_file_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clickable(val):\n",
    "    return '<a target =\"_blank\" href=\"{}\">{}</a>'.format(val,\"link\")\n",
    "    #target=\"_blank\" opens the window to new window\n",
    "    #can use val inplace to \"link\" to print the exact link\n",
    "def color_negative_red(val):\n",
    "    \"\"\"\n",
    "    Takes a scalar and returns a string with\n",
    "    the css property `'color: red'` for negative\n",
    "    strings, black otherwise.\n",
    "    \"\"\"\n",
    "    color = 'red' if val < 0 else 'green'\n",
    "    return 'color: %s' % color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the files in pandas\n",
    "def return_df(file_list,items=10):\n",
    "    for i in file_list:\n",
    "        print(30*\" \"+10*\"=\"+i+10*\"=\")\n",
    "        df=pd.read_csv(i)\n",
    "        display(df.head(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return_df(file_list=csv_file_list)\n",
    "#df=return_df(csv_file_list[0])\n",
    "#df.Styler.format({'INFO':make_clickable})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in csv_file_list:\n",
    "    df=pd.read_csv(i)\n",
    "    print(cat[i])\n",
    "    print(20*'-')\n",
    "    print(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(categories[6])\n",
    "#print(csv_file_list)\n",
    "for i,file in enumerate(csv_file_list):\n",
    "    print(i,\"\\t\",file)\n",
    "#df_large=pd.read_csv(csv_file_list[6])\n",
    "#df_large.head()\n",
    "df_oversold=pd.read_csv(csv_file_list[4])\n",
    "df_oversold.sort_values(by=['CHG%'],inplace=True,ascending=True)\n",
    "#df_oversold\n",
    "df_oversold=df_oversold.style.format({'INFO':make_clickable})\n",
    "df_oversold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(categories[6])\n",
    "#print(csv_file_list)\n",
    "for i,file in enumerate(csv_file_list):\n",
    "    print(i,\"\\t\",file)\n",
    "#df_large=pd.read_csv(csv_file_list[6])\n",
    "#df_large.head()\n",
    "df_oversold=pd.read_csv(csv_file_list[4])\n",
    "#df_oversold\n",
    "df_oversold=df_oversold.style.format({'INFO':make_clickable})\n",
    "df_oversold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #read all the dataframe and show the heading\n",
    "# data_frames=[\"df_\"+i for i in categories]\n",
    "# print(\"data_frames\\t\",data_frames)\n",
    "# for i,category in enumerate(categories):\n",
    "#     print(\"========={}==========\".format(category))\n",
    "#     data_frames[i]=pd.read_csv(category+filesuffix)\n",
    "#     #print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large-cap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "#find the ticker block\n",
    "\n",
    "header=['TICKER','LAST','CHG%','CHG','RATING','VOL','MKT CAP','P/E','EPS(TTM)','EMPLOYEES','SECTOR','INFO'] #columns name\n",
    "csv_writer=csv.writer(csv_file)\n",
    "csv_writer.writerow(header)\n",
    "\n",
    "table=soup.find('table')#table in webpagetable=soup.find('table', class_=\"tv-data-table tv-screener-table tv-screener-table--fixed\")#table in webpage\n",
    "#print(table)\n",
    "table_body=table.find('tbody',class_=\"tv-data-table__tbody\") #table row\n",
    "row_data=table_body.find_all('tr') #row data\n",
    "#print(\"row_data\\t\",row_data)\n",
    "#print(row_data.prettify())\n",
    "all_data=[]\n",
    "# tds=row_data.find_all('td')\n",
    "# td_list=[]\n",
    "web_page='https://www.tradingview.com'\n",
    "for tr in row_data:\n",
    "    tds=tr.find_all('td')\n",
    "    td_list=[]\n",
    "    for i,td in enumerate(tds):\n",
    "        print(i,td)\n",
    "        print(\"=======================\\n\")\n",
    "        if i==0:\n",
    "            #print(td)\n",
    "            #print(20*\"-\")\n",
    "            #print(\"Content of a tag\\n\")\n",
    "            print(30*\"x\")\n",
    "            symlink=web_page+td.find('a').get('href')\n",
    "            ticker=td.find('a').contents[0]\n",
    "            print(\"link\\t\",symlink)\n",
    "            print(\"TICKER\\t\",ticker)\n",
    "            print()\n",
    "            #add values to the list\n",
    "            td_list.append(ticker)\n",
    "            td_list.append(symlink)\n",
    "        else:\n",
    "            #td_list.append(td.text)\n",
    "            td_list.insert(i,td.text)\n",
    "    all_data.append(td_list)\n",
    "print(all_data)\n",
    "for i in range(len(all_data)):\n",
    "    print(all_data[i])\n",
    "    csv_writer.writerow(all_data[i])\n",
    "csv_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "#find the ticker block\n",
    "#table=soup.find('table', class_=\"tv-data-table tv-screener-table tv-screener-table--fixed\")#table in webpagetable=soup.find('table', class_=\"tv-data-table tv-screener-table tv-screener-table--fixed\")#table in webpage\n",
    "csv_file=open(\"large.csv\",\"w\")\n",
    "header=['TICKER','LAST','CHG%','CHG','RATING','VOL','MKT CAP','P/E','EPS(TTM)','EMPLOYEES','SECTOR'] #columns name\n",
    "csv_writer=csv.writer(csv_file)\n",
    "csv_writer.writerow(header)\n",
    "\n",
    "table=soup.find('table')#table in webpagetable=soup.find('table', class_=\"tv-data-table tv-screener-table tv-screener-table--fixed\")#table in webpage\n",
    "#print(table)\n",
    "table_body=table.find('tbody',class_=\"tv-data-table__tbody\") #table row\n",
    "row_data=table_body.find_all('tr') #row data\n",
    "#print(\"row_data\\t\",row_data)\n",
    "#print(row_data.prettify())\n",
    "all_data=[]\n",
    "# tds=row_data.find_all('td')\n",
    "# td_list=[]\n",
    "for tr in row_data:\n",
    "    tds=tr.find_all('td')\n",
    "    td_list=[]\n",
    "    for td in tds:\n",
    "        td_list.append(td.text)\n",
    "    all_data.append(td_list)\n",
    "print(all_data)\n",
    "for i in range(len(all_data)):\n",
    "    print(all_data[i])\n",
    "    csv_writer.writerow(all_data[i])\n",
    "csv_file.close()\n",
    "#print(row_data.text)\n",
    "#table_body=table.get()\n",
    "#table_body=table.find('tbody')\n",
    "#table_body=table.find('tbody')\n",
    "#print('table-body\\n',table_body)\n",
    "# table_data_row=table_body.find('tr')\n",
    "# table_data=table_data_row.find_all('td') #find all returns the list\n",
    "\n",
    "# #link=\"https://finance.yahoo.com\"+href\n",
    "# #print(link)\n",
    "# tick_info=[]\n",
    "# for data in table_data[:-1]:\n",
    "#     #print(\"data\\t\",data)\n",
    "    \n",
    "#     #print(\"data text\\t\",data.text)\n",
    "    \n",
    "#     tick_info.append(data.text)\n",
    "#     #tikc_info.append(link)\n",
    "# href=table_data[0].find('a').get('href')\n",
    "# #href=data.find('a').get('href')\n",
    "# link=\"https://finance.yahoo.com\"+href\n",
    "# tick_info.append(link)\n",
    "# print(\"link\\t\",link)\n",
    "# tick_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('a').get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
